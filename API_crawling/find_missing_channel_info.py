import os
import time
import csv
import requests
import pandas as pd
from dotenv import load_dotenv
from more_itertools import chunked  # pip install more-itertools

# 1. .envì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
api_keys = [
    os.environ[key]
    for key in sorted(
        [k for k in os.environ if k.startswith("API_KEY_")],
        key=lambda x: int(x.split("_")[2]) if x.split("_")[2].isdigit() else 0
    )
]
api_index = 0

def get_api_key():
    global api_index
    if api_index >= len(api_keys):
        raise RuntimeError("âŒ ëª¨ë“  API í‚¤ì˜ í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return api_keys[api_index]

def switch_api_key():
    global api_index
    api_index += 1
    if api_index < len(api_keys):
        print(f"ğŸ” API í‚¤ ì „í™˜ â†’ {api_index + 1}ë²ˆì§¸ í‚¤")
    else:
        raise RuntimeError("âŒ ë” ì´ìƒ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

def get_channel_infos_batch(channel_ids):
    while True:
        api_key = get_api_key()
        ids = ",".join(channel_ids)
        url = f"https://www.googleapis.com/youtube/v3/channels?part=snippet,statistics&id={ids}&key={api_key}"
        response = requests.get(url)

        if response.status_code == 403:
            error_reason = response.json().get("error", {}).get("errors", [{}])[0].get("reason", "")
            if error_reason in ["quotaExceeded", "userRateLimitExceeded"]:
                print(f"âš ï¸ API í‚¤ {api_index + 1} í• ë‹¹ëŸ‰ ì´ˆê³¼")
                switch_api_key()
                continue
            else:
                print(f"âŒ ì ‘ê·¼ ì˜¤ë¥˜: {response.text}")
                return []
        elif response.status_code != 200:
            print(f"âŒ ê¸°íƒ€ ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return []

        results = []
        for item in response.json().get("items", []):
            snippet = item['snippet']
            stats = item['statistics']
            results.append({
                "channelID": item['id'],
                "channelTitle": snippet.get('title', ''),
                "subscriberCount": stats.get('subscriberCount', '0'),
                "totalViews": stats.get('viewCount', '0'),
                "videosCount": stats.get('videoCount', '0')
            })
        return results

# 2. ëˆ„ë½ëœ IDë§Œ í•„í„°ë§
input_path = "C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/missing_channelIDs_2.csv"
output_path = "C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/missing_channels2_info.csv"

# ì „ì²´ ìˆ˜ì§‘ ëŒ€ìƒ
missing_df = pd.read_csv(input_path)
target_ids = set(missing_df['missing_channelID'].dropna().unique())

# ì´ë¯¸ ìˆ˜ì§‘ëœ ê²ƒ ë¶ˆëŸ¬ì˜¤ê¸°
if os.path.exists(output_path):
    collected_df = pd.read_csv(output_path)
    collected_ids = set(collected_df['channelID'].dropna().unique())
else:
    collected_ids = set()
    with open(output_path, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=["channelID", "channelTitle", "subscriberCount", "totalViews", "videosCount"])
        writer.writeheader()

# ë‚¨ì€ ê²ƒë§Œ ì²˜ë¦¬
remaining_ids = list(target_ids - collected_ids)
print(f"ğŸ§© ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ì±„ë„ ìˆ˜: {len(remaining_ids)}")

# 3. ë°°ì¹˜ ë‹¨ìœ„ë¡œ API í˜¸ì¶œ
with open(output_path, mode='a', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=["channelID", "channelTitle", "subscriberCount", "totalViews", "videosCount"])
    for i, batch in enumerate(chunked(remaining_ids, 50)):
        print(f"ğŸ” Batch {i+1}/{len(remaining_ids)//50 + 1} â†’ {len(batch)}ê°œ ì±„ë„ ìš”ì²­")
        try:
            results = get_channel_infos_batch(batch)
            for info in results:
                writer.writerow(info)
            f.flush()
        except RuntimeError as e:
            print(str(e))
            break
        time.sleep(1.1)  # ì†ë„ ì œì–´
