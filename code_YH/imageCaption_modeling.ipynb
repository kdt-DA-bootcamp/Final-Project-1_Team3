{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 키워드 전처리/임베딩 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imgKw 전처리 과정\n",
    "## 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\yh\\anaconda3\\envs\\model\\lib\\site-packages (1.1.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pymysql in c:\\users\\yh\\anaconda3\\envs\\model\\lib\\site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yttest1234\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일의 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 DB 암호 가져오기 (여기서는 변수명이 '1P_DB_PW'라고 가정)\n",
    "db_password = os.getenv('1P_DB_PW')\n",
    "print(db_password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장방식을 .csv가 아니라 .pkl (피클)을 쓴 이유\n",
    "\n",
    "장점\n",
    "빠르다text 파일을 사용하는 경우 필요한 부분들을 파싱해야 하지만 pickle은 이미 필요한 형태대로 저장이 되어 있기 때문에 훨씬 빠르다고 한다.\n",
    " \n",
    "\n",
    "주의점\n",
    "안전하지 않다[2]pickle module은 안전하지 않고 unpickle data만 신뢰할 수 있다고 한다.RCE(원격코드실행) 공격을 받을 수 있어, 다운 받은 pickle 파일을 사용할 때 매우 주의해야 한다.\n",
    " \n",
    "검증 되지 않은 pickle data를 unpicking하게 될 때 임의의 코드가 실행될 수 있다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumbnailURL 열에 결측치가 없습니다.\n",
      "                                               imgKw  \\\n",
      "0  Sports venue, Baseball field, Baseball, Bat-an...   \n",
      "1  Black hair, Mouth, Facial expression, News, Lo...   \n",
      "2  Facial expression, News, Screenshot, Person, S...   \n",
      "3  News, Lunch, Happiness, Screenshot, Comfort fo...   \n",
      "4  Facial expression, Gesture, Screenshot, Cosmet...   \n",
      "\n",
      "                                    cleaned_keywords  \n",
      "0  [sports venue, baseball field, baseball, batan...  \n",
      "1  [black hair, mouth, facial expression, news, l...  \n",
      "2  [facial expression, news, screenshot, person, ...  \n",
      "3  [news, lunch, happiness, screenshot, comfort f...  \n",
      "4  [facial expression, gesture, screenshot, cosme...  \n",
      "전처리된 데이터가 .pkl 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# DB 접속 설정\n",
    "db_config = {\n",
    "    'host': '121.128.172.79',\n",
    "    'user': 'user3',\n",
    "    'password': db_password,  # db_password 변수에 비밀번호를 지정하세요.\n",
    "    'database': 'yttest_db',\n",
    "    'port': 3306,\n",
    "    'cursorclass': pymysql.cursors.DictCursor\n",
    "}\n",
    "\n",
    "# DB 연결 및 데이터 불러오기\n",
    "conn = pymysql.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "# video 테이블에서 thumbnailURL, imgKw, likeCount, viewCount, commentCount, catagoryID 컬럼을 가져옴\n",
    "query = \"\"\"SELECT v.thumbnailURL, t.imgKw, v.likeCount, v.viewCount, v.commentCount, v.categoryID, v.videoID\n",
    "            FROM video v\n",
    "            JOIN thumbnail t ON v.thumbnailURL = t.thumbnailURL;\"\"\"\n",
    "cursor.execute(query)\n",
    "data = cursor.fetchall()\n",
    "conn.close()  # 데이터 불러온 후 연결 종료\n",
    "\n",
    "# DataFrame 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 결측치 처리: imgKw 컬럼의 결측값을 'unknown'으로 채워 넣기\n",
    "df['imgKw'] = df['imgKw'].fillna('unknown')\n",
    "\n",
    "# thumbnailURL 결측치 확인\n",
    "if df['thumbnailURL'].isnull().sum() > 0:\n",
    "    print(\"thumbnailURL 열에 결측치가 존재합니다. 추가 전처리가 필요합니다.\")\n",
    "else:\n",
    "    print(\"thumbnailURL 열에 결측치가 없습니다.\")\n",
    "\n",
    "# 토큰 리스트 전처리 함수 정의\n",
    "def clean_tokenized_keywords(token_str):\n",
    "    if pd.isnull(token_str):\n",
    "        return []\n",
    "    token_str = token_str.strip()\n",
    "    try:\n",
    "        # 만약 token_str이 리스트 리터럴 형태라면 ast.literal_eval 시도\n",
    "        if token_str.startswith('[') and token_str.endswith(']'):\n",
    "            tokens = ast.literal_eval(token_str)\n",
    "        else:\n",
    "            # 그렇지 않으면 쉼표로 분리\n",
    "            tokens = token_str.split(',')\n",
    "    except Exception as e:\n",
    "        # 변환 실패 시 쉼표로 분리\n",
    "        tokens = token_str.split(',')\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        # 불필요한 기호 제거 및 소문자화\n",
    "        token = token.strip(\"[]'\\\", \")\n",
    "        token = re.sub(r'[^\\w\\s]', '', token)\n",
    "        token = token.strip()\n",
    "        if token:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "# 불용어 설정 (예: 'youtube', 'top', 'video', 'meter' 등)\n",
    "stopwords = {'youtube', 'top', 'video', 'meter'}\n",
    "\n",
    "def filter_stopwords(tokens, stopwords):\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "# imgKw 컬럼에 전처리 적용하여 cleaned_keywords 열 생성\n",
    "df['cleaned_keywords'] = df['imgKw'].apply(clean_tokenized_keywords)\n",
    "df['cleaned_keywords'] = df['cleaned_keywords'].apply(lambda tokens: filter_stopwords(tokens, stopwords))\n",
    "\n",
    "# 결과 확인 (예시)\n",
    "print(df[['imgKw', 'cleaned_keywords']].head())\n",
    "\n",
    "# 저장 경로 (pkl 파일로 저장)\n",
    "pkl_output_path = r\"C:\\Users\\YH\\Desktop\\데이터분석가부트캠프\\practice\\HAYEONGHAN705\\1st_project\\csv\\csv_model\\thumbnail_keywords_cleaned.pkl\"\n",
    "\n",
    "# DataFrame을 pickle 형식으로 저장 (모든 컬럼 포함: thumbnailURL, imgKw, cleaned_keywords, likeCount, viewCount, commentCount)\n",
    "df.to_pickle(pkl_output_path)\n",
    "\n",
    "print(\"전처리된 데이터가 .pkl 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101396 entries, 0 to 101395\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   thumbnailURL      101396 non-null  object\n",
      " 1   imgKw             101396 non-null  object\n",
      " 2   likeCount         101396 non-null  int64 \n",
      " 3   viewCount         101396 non-null  int64 \n",
      " 4   commentCount      101396 non-null  int64 \n",
      " 5   categoryID        101396 non-null  object\n",
      " 6   videoID           101396 non-null  object\n",
      " 7   cleaned_keywords  101396 non-null  object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 6.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert(kobert)모델로 임베딩하면서 토큰화 동시 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101396 entries, 0 to 101395\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   thumbnailURL      101396 non-null  object\n",
      " 1   imgKw             101396 non-null  object\n",
      " 2   likeCount         101396 non-null  int64 \n",
      " 3   viewCount         101396 non-null  int64 \n",
      " 4   commentCount      101396 non-null  int64 \n",
      " 5   categoryID        101396 non-null  object\n",
      " 6   cleaned_keywords  101396 non-null  object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 5.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# pickle 파일에서 DataFrame 불러오기\n",
    "import pickle\n",
    "\n",
    "# pkl_file_path = r\"C:\\Users\\YH\\Desktop\\데이터분석가부트캠프\\practice\\HAYEONGHAN705\\1st_project\\csv\\thumbnail_keywords_cleaned.pkl\"\n",
    "# with open(pkl_file_path, 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "# 위 방식보다 아래 방식으로 데이터를 불러오는게 더 코드가 간결함\n",
    "data = pd.read_pickle(r\"C:\\Users\\YH\\Desktop\\데이터분석가부트캠프\\practice\\HAYEONGHAN705\\1st_project\\csv\\csv_model\\thumbnail_keywords_cleaned.pkl\")  # 파일 경로 수정\n",
    "\n",
    "    \n",
    "print(data.info()) #[\"cleaned_keywords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YH\\anaconda3\\envs\\model\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import ast\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 장치: cuda\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 로드 (bert-base-uncased 사용)\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"사용 장치:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_keywords의 BERT 임베딩 행렬 크기: (101396, 768)\n"
     ]
    }
   ],
   "source": [
    "# 토큰 파싱 함수 (중복된 함수 정의는 제거)\n",
    "def parse_token_list(token_value):\n",
    "    if isinstance(token_value, (list, np.ndarray)):\n",
    "        return list(token_value)\n",
    "    if pd.isnull(token_value):\n",
    "        return []\n",
    "    try:\n",
    "        tokens = ast.literal_eval(token_value)\n",
    "        return tokens if isinstance(tokens, list) else []\n",
    "    except Exception:\n",
    "        return token_value.split()\n",
    "\n",
    "# BERT 임베딩 추출 함수\n",
    "def get_bert_embedding_keywords(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    for key, value in inputs.items():\n",
    "        inputs[key] = value.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return cls_embedding[0].cpu().numpy()\n",
    "\n",
    "# cleaned_keywords 열에 대해 임베딩 추출\n",
    "all_keyword_embeddings = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    keywords_list = parse_token_list(row['cleaned_keywords'])\n",
    "    if len(keywords_list) == 0:\n",
    "        embedding = np.zeros(model.config.hidden_size)\n",
    "    else:\n",
    "        combined_str = \" \".join(keywords_list)\n",
    "        embedding = get_bert_embedding_keywords(combined_str)\n",
    "    all_keyword_embeddings.append(embedding)\n",
    "\n",
    "X_keywords = np.vstack(all_keyword_embeddings)\n",
    "print(\"cleaned_keywords의 BERT 임베딩 행렬 크기:\", X_keywords.shape)\n",
    "\n",
    "# DataFrame에 벡터 추가\n",
    "data['bert_keyword_vector'] = list(all_keyword_embeddings)\n",
    "\n",
    "# 아래는 .pkl방식으로 저장한다면 쓸 필요X. .csv 파일로 저장할때만 사용\n",
    "# import json\n",
    "\n",
    "# def vector_to_json(vector):\n",
    "#     if isinstance(vector, np.ndarray):\n",
    "#         vector = vector.tolist()\n",
    "#     return json.dumps(vector)\n",
    "\n",
    "# data['bert_keyword_vector_json'] = data['bert_keyword_vector'].apply(vector_to_json)\n",
    "\n",
    "# 최종 결과를 pickle 파일로 저장 (또는 CSV도 함께 저장 가능)\n",
    "data.to_pickle(r\"C:\\Users\\YH\\Desktop\\데이터분석가부트캠프\\practice\\HAYEONGHAN705\\1st_project\\csv\\csv_model\\merged_data_vector_0404.pkl\")\n",
    "# data.to_csv(r\"/path/to/your/merged_data_vector_0401.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습을 위한 데이터 셋 생성(likeCount, viewCount, commentCount 반영)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         5\n",
      "1         9\n",
      "2         1\n",
      "3         1\n",
      "4         6\n",
      "         ..\n",
      "101391    4\n",
      "101392    3\n",
      "101393    6\n",
      "101394    1\n",
      "101395    4\n",
      "Name: categoryID, Length: 101396, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data[\"categoryID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [-0.34264722, 0.033941608, -0.2915058, -0.0591...\n",
      "1    [-0.17848471, -0.083290905, -0.15581135, -0.52...\n",
      "2    [-0.118732065, 0.06530086, 0.013026365, -0.003...\n",
      "3    [-0.2882179, -0.27976537, -0.12729499, -0.1137...\n",
      "4    [-0.5027794, 0.083117984, -0.51912737, -0.2629...\n",
      "Name: bert_keyword_vector, dtype: object\n",
      "0    <class 'numpy.ndarray'>\n",
      "1    <class 'numpy.ndarray'>\n",
      "2    <class 'numpy.ndarray'>\n",
      "3    <class 'numpy.ndarray'>\n",
      "4    <class 'numpy.ndarray'>\n",
      "Name: bert_keyword_vector, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# pickle 파일에서 데이터 불러오기\n",
    "data = pd.read_pickle(r\"C:\\Users\\YH\\Desktop\\데이터분석가부트캠프\\practice\\HAYEONGHAN705\\1st_project\\csv\\csv_model\\merged_data_vector_0404.pkl\")\n",
    "\n",
    "# 만약 'bert_keyword_vector' 열이 이미 numpy array 형태라면 별도 변환은 필요하지 않습니다.\n",
    "# 만약 JSON 문자열이라면 변환하는 함수를 사용하세요.\n",
    "# def convert_embedding(x):\n",
    "#     if isinstance(x, (list, np.ndarray)):\n",
    "#         return np.array(x)\n",
    "#     if pd.isnull(x):\n",
    "#         return None\n",
    "#     try:\n",
    "#         # 우선 JSON 파싱 시도\n",
    "#         return np.array(json.loads(x))\n",
    "#     except Exception:\n",
    "#         # JSON 파싱 실패 시 ast.literal_eval 시도\n",
    "#         import ast\n",
    "#         try:\n",
    "#             return np.array(ast.literal_eval(x))\n",
    "#         except Exception:\n",
    "#             return None\n",
    "\n",
    "# # 예시: 만약 'bert_keyword_vector_json' 열이 존재한다면 이를 사용하여 변환\n",
    "# if 'bert_keyword_vector_json' in data.columns:\n",
    "#     data['bert_keyword_vector'] = data['bert_keyword_vector_json'].apply(convert_embedding)\n",
    "\n",
    "# 확인\n",
    "print(data['bert_keyword_vector'].head())\n",
    "print(data['bert_keyword_vector'].apply(lambda x: type(x)).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_keywords shape: (101396, 768)\n"
     ]
    }
   ],
   "source": [
    "# 기존 코드: (N, 768)\n",
    "X_keywords = np.vstack(data['bert_keyword_vector'].values)\n",
    "print(\"X_keywords shape:\", X_keywords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101396 entries, 0 to 101395\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   thumbnailURL         101396 non-null  object\n",
      " 1   imgKw                101396 non-null  object\n",
      " 2   likeCount            101396 non-null  int64 \n",
      " 3   viewCount            101396 non-null  int64 \n",
      " 4   commentCount         101396 non-null  int64 \n",
      " 5   categoryID           101396 non-null  object\n",
      " 6   cleaned_keywords     101396 non-null  object\n",
      " 7   bert_keyword_vector  101396 non-null  object\n",
      " 8   videoID              101396 non-null  object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 7.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_keywords shape: (101396, 768)\n",
      "X_keywords_reduced shape: (101396, 512)\n",
      "X_keywords shape: (101396, 768)\n",
      "Unique y_class: [0 1 2 3 4 5 6 7 8 9]\n",
      "학습 데이터 X_train shape: (81116, 512)\n",
      "학습 데이터 y_reg_train shape: (81116, 3)\n",
      "학습 데이터 y_class_train shape: (81116,)\n",
      "Unique y_class_train: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # 각 동영상의 임베딩을 하나의 2차원 배열로 결합 (예: (N, 768))\n",
    "# X_keywords = np.vstack(data['bert_keyword_vector'].values)\n",
    "# print(\"X_keywords shape:\", X_keywords.shape)\n",
    "\n",
    "# 기존 코드: (N, 768)\n",
    "X_keywords = np.vstack(data['bert_keyword_vector'].values)\n",
    "print(\"X_keywords shape:\", X_keywords.shape)\n",
    "\n",
    "# 추가: PCA로 768차원을 512차원으로 축소\n",
    "pca = PCA(n_components=512)\n",
    "X_keywords_reduced = pca.fit_transform(X_keywords)\n",
    "print(\"X_keywords_reduced shape:\", X_keywords_reduced.shape)\n",
    "\n",
    "\n",
    "# 예시 DataFrame: data\n",
    "# data에는 videoID, categoryID, viewCount, likeCount, commentCount, bert_keyword_vector 열이 있다고 가정합니다.\n",
    "# bert_keyword_vector 열은 이미 각 행에 대해 BERT 임베딩(예: 768차원 벡터)이 저장되어 있음\n",
    "\n",
    "# 1) 임베딩 벡터 추출\n",
    "# 예를 들어, data['bert_keyword_vector']가 리스트나 numpy 배열 형태라고 가정\n",
    "X_keywords = np.vstack(data['bert_keyword_vector'].values)\n",
    "print(\"X_keywords shape:\", X_keywords.shape)\n",
    "\n",
    "# 2) 회귀용 타겟: viewCount, likeCount, commentCount\n",
    "y_view = data['viewCount'].values\n",
    "y_like = data['likeCount'].values\n",
    "y_comment = data['commentCount'].values\n",
    "y_reg = np.stack([y_view, y_like, y_comment], axis=1)  # shape: (N, 3)\n",
    "\n",
    "# 3) 분류용 타겟: categoryID를 재매핑 (원래 1~10 → 0~9)\n",
    "data['categoryID'] = data['categoryID'].astype(int)\n",
    "y_class = data['categoryID'].values - 1\n",
    "print(\"Unique y_class:\", np.unique(y_class))  # [0 1 2 3 4 5 6 7 8 9]가 출력되어야 함\n",
    "\n",
    "# 4) videoID 보관 (후처리 및 결과 해석용)\n",
    "video_ids = data['videoID'].values\n",
    "\n",
    "# # 5) Train-Test split (예: 80% 학습, 20% 테스트)\n",
    "# X_train, X_test, y_reg_train, y_reg_test, y_class_train, y_class_test, vid_train, vid_test = train_test_split(\n",
    "#     X_keywords, y_reg, y_class, video_ids, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# 기존 코드에서 X_keywords 대신 X_keywords_reduced 사용\n",
    "\n",
    "X_train, X_test, y_reg_train, y_reg_test, y_class_train, y_class_test, vid_train, vid_test = train_test_split(\n",
    "    X_keywords_reduced, y_reg, y_class, video_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"학습 데이터 X_train shape:\", X_train.shape)\n",
    "print(\"학습 데이터 y_reg_train shape:\", y_reg_train.shape)\n",
    "print(\"학습 데이터 y_class_train shape:\", y_class_train.shape)\n",
    "print(\"Unique y_class_train:\", np.unique(y_class_train))  # [0 1 2 3 4 5 6 7 8 9]가 출력되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 카테고리 개수: 10\n",
      "y_class_train unique values: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Label encoding은 이미 실행했다고 가정합니다.\n",
    "# 예시:\n",
    "encoder = LabelEncoder()\n",
    "data['categoryID_encoded'] = encoder.fit_transform(data['categoryID'])\n",
    "y_class = data['categoryID_encoded'].values\n",
    "\n",
    "# 데이터 정제: categoryID_encoded 열에 결측치가 있다면 제거\n",
    "data = data.dropna(subset=['categoryID_encoded'])\n",
    "data['categoryID_encoded'] = data['categoryID_encoded'].astype(int)\n",
    "\n",
    "# 재매핑된 y_class를 사용합니다.\n",
    "y_class = data['categoryID_encoded'].values\n",
    "num_categories = len(np.unique(y_class))\n",
    "print(\"새로운 카테고리 개수:\", num_categories)\n",
    "\n",
    "# 이후 train_test_split 과정에서, 정제된 y_class를 사용합니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 여기서 X_keywords 대신 PCA로 축소한 X_keywords_reduced를 사용합니다.\n",
    "X_train, X_test, y_reg_train, y_reg_test, y_class_train, y_class_test, vid_train, vid_test = train_test_split(\n",
    "    X_keywords_reduced, y_reg, y_class, video_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"y_class_train unique values:\", np.unique(y_class_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_categories):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        # 회귀를 위한 예시 레이어\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 3)  # viewCount, likeCount, commentCount 예측\n",
    "        )\n",
    "        # 분류를 위한 예시 레이어\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_categories)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pred_reg = self.regressor(x)\n",
    "        pred_class = self.classifier(x)\n",
    "        return pred_reg, pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 35732317836.1339\n",
      "Epoch 2/20, Loss: 35564997212.6994\n",
      "Epoch 3/20, Loss: 35327751825.7839\n",
      "Epoch 4/20, Loss: 35093194202.7366\n",
      "Epoch 5/20, Loss: 34882191706.5804\n",
      "Epoch 6/20, Loss: 34708237890.7583\n",
      "Epoch 7/20, Loss: 34575299648.6659\n",
      "Epoch 8/20, Loss: 34480485518.2729\n",
      "Epoch 9/20, Loss: 34412088246.1818\n",
      "Epoch 10/20, Loss: 34364316052.3560\n",
      "Epoch 11/20, Loss: 34329448184.4825\n",
      "Epoch 12/20, Loss: 34304156090.8084\n",
      "Epoch 13/20, Loss: 34283066262.3254\n",
      "Epoch 14/20, Loss: 34265366097.4999\n",
      "Epoch 15/20, Loss: 34250544232.7026\n",
      "Epoch 16/20, Loss: 34237599306.7840\n",
      "Epoch 17/20, Loss: 34225364999.3850\n",
      "Epoch 18/20, Loss: 34214922501.8827\n",
      "Epoch 19/20, Loss: 34205105705.5452\n",
      "Epoch 20/20, Loss: 34195631795.0637\n",
      "데이터셋 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# TensorDataset 구성\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_reg_train_tensor = torch.FloatTensor(y_reg_train)\n",
    "y_class_train_tensor = torch.LongTensor(y_class_train)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_reg_train_tensor, y_class_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 모델 초기화 (input_dim = X_train.shape[1] -> 512)\n",
    "input_dim = X_train.shape[1]  # 512\n",
    "hidden_dim = 256\n",
    "model = MultiTaskModel(input_dim, hidden_dim, num_categories)\n",
    "# 손실 함수 정의 (회귀와 분류 각각)\n",
    "criterion_reg = nn.MSELoss()\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer 및 학습 관련 변수 정의)\n",
    "num_epochs = 20\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y_reg, batch_y_class in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred_reg, pred_class = model(batch_X)\n",
    "        loss_reg = criterion_reg(pred_reg, batch_y_reg)\n",
    "        loss_class = criterion_class(pred_class, batch_y_class)\n",
    "        loss = loss_reg + loss_class\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# 예를 들어, train_test_split 결과를 저장합니다.\n",
    "dataset = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_reg_train': y_reg_train,\n",
    "    'y_reg_test': y_reg_test,\n",
    "    'y_class_train': y_class_train,\n",
    "    'y_class_test': y_class_test,\n",
    "    'vid_train': vid_train,\n",
    "    'vid_test': vid_test\n",
    "}\n",
    "\n",
    "data.to_pickle(r\"C:\\Users\\YH\\Desktop\\데이터분석가부트캠프\\practice\\HAYEONGHAN705\\1st_project\\csv\\csv_model\\merged_data_set_0404.pkl\")\n",
    "\n",
    "print(\"데이터셋 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 불러오기 완료!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "data = pd.read_pickle(r\"C:\\Users\\YH\\Desktop\\데이터분석가부트캠프\\practice\\HAYEONGHAN705\\1st_project\\csv\\csv_model\\merged_data_set_0404.pkl\")\n",
    "\n",
    "X_train = dataset['X_train']\n",
    "X_test = dataset['X_test']\n",
    "y_reg_train = dataset['y_reg_train']\n",
    "y_reg_test = dataset['y_reg_test']\n",
    "y_class_train = dataset['y_class_train']\n",
    "y_class_test = dataset['y_class_test']\n",
    "vid_train = dataset['vid_train']\n",
    "vid_test = dataset['vid_test']\n",
    "\n",
    "print(\"데이터셋 불러오기 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추천 키워드: [('person', np.float64(0.05760085594020606)), ('food', np.float64(0.0562537281642235)), ('clothing', np.float64(0.04886552116566644)), ('screenshot', np.float64(0.040348741531132236)), ('facial', np.float64(0.034908806413900116)), ('expression', np.float64(0.03378454926732957)), ('news', np.float64(0.030894648014358293)), ('happiness', np.float64(0.028161077297271322)), ('film', np.float64(0.026103472383160204)), ('game', np.float64(0.025982370991353414)), ('advertising', np.float64(0.022682146418673513)), ('cuisine', np.float64(0.019846972956997926)), ('hair', np.float64(0.019562997563047625)), ('television', np.float64(0.019508443897107375)), ('cartoon', np.float64(0.019251372953768376)), ('device', np.float64(0.018994789071219407)), ('outerwear', np.float64(0.018806019001232407)), ('animation', np.float64(0.01869022203133106)), ('recipe', np.float64(0.01607365240222632)), ('show', np.float64(0.015992404881143948))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 인기 영상 필터링 (예: 상위 20% 영상만 선택)\n",
    "popularity = data['viewCount'] + data['likeCount'] + data['commentCount']\n",
    "threshold = np.percentile(popularity, 80)\n",
    "# 슬라이스를 복사하여 사용 (SettingWithCopyWarning 해결)\n",
    "popular_videos = data[popularity >= threshold].copy()\n",
    "\n",
    "# NaN 값을 빈 문자열로 대체한 후, 각 동영상의 tokenized_keywords를 하나의 문자열로 변환\n",
    "popular_videos['keywords_str'] = popular_videos['cleaned_keywords'].fillna(\"\").apply(\n",
    "    lambda x: \" \".join(x) if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(popular_videos['keywords_str'])\n",
    "\n",
    "# 각 단어의 평균 TF-IDF 점수 계산 (카테고리별로 나누어 할 수도 있음)\n",
    "avg_tfidf = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "keywords = tfidf_vectorizer.get_feature_names_out()\n",
    "keyword_scores = list(zip(keywords, avg_tfidf))\n",
    "# 점수가 높은 상위 20개 단어를 추천 키워드로 선택\n",
    "recommended_keywords = sorted(keyword_scores, key=lambda x: x[1], reverse=True)[:20]\n",
    "print(\"추천 키워드:\", recommended_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카테고리 추천 키워드:\n",
      "car: 0.1428\n",
      "vehicle: 0.1057\n",
      "kia: 0.0646\n",
      "automotive: 0.0608\n",
      "sport: 0.0578\n",
      "luxury: 0.0574\n",
      "utility: 0.0571\n",
      "bmw: 0.0551\n",
      "wheel: 0.0504\n",
      "hyundai: 0.0478\n",
      "device: 0.0440\n",
      "tire: 0.0432\n",
      "person: 0.0422\n",
      "door: 0.0391\n",
      "suv: 0.0351\n",
      "clothing: 0.0345\n",
      "toyota: 0.0331\n",
      "compact: 0.0316\n",
      "rover: 0.0315\n",
      "display: 0.0314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 우선, 기존에 인기 영상(popular_videos)이 설정되어 있다고 가정합니다.\n",
    "# 예: popular_videos = data[popularity >= threshold].copy()\n",
    "\n",
    "# 1. 카테고리 3(예: '여행')에 해당하는 인기 영상만 필터링합니다.\n",
    "# 여기서는 원본 data의 'categoryID'가 사용된다고 가정합니다.\n",
    "popular_videos_cat3 = popular_videos[popular_videos['categoryID'] == 2].copy()\n",
    "\n",
    "# 2. NaN 값을 빈 문자열로 대체하고, tokenized_keywords를 하나의 문자열로 결합합니다.\n",
    "popular_videos_cat3['keywords_str'] = popular_videos_cat3['cleaned_keywords'].fillna(\"\").apply(\n",
    "    lambda x: \" \".join(x) if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "# 3. TF-IDF 벡터화: 새로 객체를 생성하여 카테고리 3 전용 TF-IDF 행렬을 만듭니다.\n",
    "tfidf_vectorizer_cat3 = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix_cat3 = tfidf_vectorizer_cat3.fit_transform(popular_videos_cat3['keywords_str'])\n",
    "\n",
    "# 4. 각 단어의 평균 TF-IDF 점수를 계산합니다.\n",
    "avg_tfidf_cat3 = np.mean(tfidf_matrix_cat3.toarray(), axis=0)\n",
    "\n",
    "# 5. 단어 목록과 TF-IDF 점수를 짝지어 정렬합니다.\n",
    "keywords_cat3 = tfidf_vectorizer_cat3.get_feature_names_out()\n",
    "keyword_scores_cat3 = list(zip(keywords_cat3, avg_tfidf_cat3))\n",
    "recommended_keywords_cat3 = sorted(keyword_scores_cat3, key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "print(\"카테고리 추천 키워드:\")\n",
    "for kw, score in recommended_keywords_cat3:\n",
    "    print(f\"{kw}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (101396, 768)\n",
      "Vocabulary 크기: 500\n",
      "y_keywords shape: (101396, 500)\n"
     ]
    }
   ],
   "source": [
    "# 1. 입력 특성 X 생성\n",
    "# 각 동영상의 임베딩 벡터가 모두 잘 복원되어 있다고 가정합니다.\n",
    "X = np.vstack(data['bert_keyword_vector'].dropna().values)  # shape: (N, 768)\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "# 2. 타겟 생성: tokenized_keywords 열에서 candidate vocabulary 선정\n",
    "from collections import Counter\n",
    "\n",
    "all_keywords = []\n",
    "# tokenized_keywords가 리스트형태로 저장되어 있다고 가정합니다.\n",
    "for kw_list in data['cleaned_keywords']:\n",
    "    if isinstance(kw_list, list):\n",
    "        all_keywords.extend(kw_list)\n",
    "    elif isinstance(kw_list, str):\n",
    "        # 만약 문자열 형태라면, 공백 기준 분할\n",
    "        all_keywords.extend(kw_list.split())\n",
    "\n",
    "# 상위 500개 단어 선택\n",
    "vocab_counter = Counter(all_keywords)\n",
    "vocab = [kw for kw, cnt in vocab_counter.most_common(500)]\n",
    "V = len(vocab)\n",
    "print(\"Vocabulary 크기:\", V)\n",
    "\n",
    "# 함수: 각 동영상마다 multi-hot 벡터 생성\n",
    "def create_target_vector(kw_list, vocab):\n",
    "    target = np.zeros(len(vocab), dtype=np.float32)\n",
    "    if isinstance(kw_list, list):\n",
    "        for word in kw_list:\n",
    "            if word in vocab:\n",
    "                # vocab 내 단어의 인덱스를 찾아서 1로 설정\n",
    "                idx = vocab.index(word)\n",
    "                target[idx] = 1.0\n",
    "    elif isinstance(kw_list, str):\n",
    "        # 문자열인 경우 split 후 처리\n",
    "        tokens = kw_list.split()\n",
    "        for word in tokens:\n",
    "            if word in vocab:\n",
    "                idx = vocab.index(word)\n",
    "                target[idx] = 1.0\n",
    "    return target\n",
    "\n",
    "# 각 동영상에 대해 multi-label 타겟 생성\n",
    "# NaN 처리: NaN이면 빈 리스트로 간주\n",
    "# 각 동영상에 대해 multi-label 타겟 생성\n",
    "y_keywords = np.array([\n",
    "    create_target_vector(kw_list, vocab) if isinstance(kw_list, (list, str))\n",
    "    else np.zeros(V, dtype=np.float32)\n",
    "    for kw_list in data['cleaned_keywords']\n",
    "])\n",
    "print(\"y_keywords shape:\", y_keywords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (81116, 768) X_val shape: (20280, 768)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# train/validation 분할 (예: 80/20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_keywords, test_size=0.2, random_state=42)\n",
    "print(\"X_train shape:\", X_train.shape, \"X_val shape:\", X_val.shape)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class KeywordRecommender(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(KeywordRecommender, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)  # 출력: 각 단어의 로짓 (logits)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "input_dim = X_train.shape[1]  # 예: 768\n",
    "hidden_dim = 256\n",
    "output_dim = V  # vocabulary 크기 (예: 500)\n",
    "model = KeywordRecommender(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0433\n",
      "Epoch 2/20, Loss: 0.0298\n",
      "Epoch 3/20, Loss: 0.0273\n",
      "Epoch 4/20, Loss: 0.0260\n",
      "Epoch 5/20, Loss: 0.0252\n",
      "Epoch 6/20, Loss: 0.0246\n",
      "Epoch 7/20, Loss: 0.0241\n",
      "Epoch 8/20, Loss: 0.0237\n",
      "Epoch 9/20, Loss: 0.0234\n",
      "Epoch 10/20, Loss: 0.0232\n",
      "Epoch 11/20, Loss: 0.0230\n",
      "Epoch 12/20, Loss: 0.0228\n",
      "Epoch 13/20, Loss: 0.0226\n",
      "Epoch 14/20, Loss: 0.0225\n",
      "Epoch 15/20, Loss: 0.0224\n",
      "Epoch 16/20, Loss: 0.0223\n",
      "Epoch 17/20, Loss: 0.0221\n",
      "Epoch 18/20, Loss: 0.0220\n",
      "Epoch 19/20, Loss: 0.0220\n",
      "Epoch 20/20, Loss: 0.0219\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)  # outputs shape: (batch_size, V)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 및 관련 정보가 한 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 관련 정보를 하나의 딕셔너리에 저장\n",
    "save_dict = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab': vocab,\n",
    "    'input_dim': input_dim,    # 예: 768\n",
    "    'hidden_dim': hidden_dim,  # 예: 256\n",
    "    'output_dim': output_dim   # len(vocab)\n",
    "}\n",
    "\n",
    "save_path = r\"C:\\Users\\YH\\Desktop\\데이터분석가부트캠프\\practice\\HAYEONGHAN705\\1st_project\\csv\\csv_model\\keyword_recommender_full.pth\"\n",
    "torch.save(save_dict, save_path)\n",
    "print(\"모델 및 관련 정보가 한 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 및 관련 정보 불러오기 완료!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "\n",
    "# 모델 구조 정의 (저장할 때와 동일하게)\n",
    "class KeywordRecommender(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(KeywordRecommender, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)  # 출력: 각 단어의 로짓\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 저장된 딕셔너리 불러오기\n",
    "save_path = r\"C:\\Users\\YH\\Desktop\\데이터분석가부트캠프\\practice\\HAYEONGHAN705\\1st_project\\csv\\csv_model\\keyword_recommender_full.pth\"\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# 저장된 정보로 모델 초기화\n",
    "input_dim = checkpoint['input_dim']\n",
    "hidden_dim = checkpoint['hidden_dim']\n",
    "output_dim = checkpoint['output_dim']\n",
    "vocab = checkpoint['vocab']\n",
    "\n",
    "model = KeywordRecommender(input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"모델 및 관련 정보 불러오기 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카테고리 5 추천 키워드:\n",
      "person: 0.7982\n",
      "clothing: 0.5756\n",
      "sports venue: 0.3134\n",
      "sports: 0.2641\n",
      "jersey: 0.1826\n",
      "sports uniform: 0.1495\n",
      "player: 0.1491\n",
      "screenshot: 0.1368\n",
      "stadium: 0.1156\n",
      "advertising: 0.1102\n",
      "shorts: 0.1056\n",
      "baseball: 0.1055\n",
      "soccer player: 0.1021\n",
      "logo: 0.1020\n",
      "hat: 0.0999\n",
      "uniform: 0.0969\n",
      "sports fan jersey: 0.0942\n",
      "football player: 0.0941\n",
      "football: 0.0883\n",
      "cap: 0.0803\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 4. 추론 실행 (예: 카테고리 10의 추천 키워드 추출)\n",
    "# 원본 데이터(data)에서 카테고리 10을 선택하고, 임베딩 벡터를 모읍니다.\n",
    "selected_category = 5\n",
    "subset = data[data['categoryID'] == selected_category].copy()\n",
    "subset = subset.dropna(subset=['bert_keyword_vector'])\n",
    "X_cat = np.vstack(subset['bert_keyword_vector'].values)\n",
    "X_cat_tensor = torch.FloatTensor(X_cat)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(X_cat_tensor)\n",
    "    preds = torch.sigmoid(preds).cpu().numpy()\n",
    "\n",
    "# 동영상별 예측 결과(각 행 벡터)를 평균하여 카테고리 전체의 추천 점수를 산출\n",
    "avg_preds = np.mean(preds, axis=0)  # shape: (output_dim,)\n",
    "\n",
    "# vocabulary와 점수를 짝지어 상위 추천 키워드 선정\n",
    "keyword_scores = list(zip(vocab, avg_preds))\n",
    "recommended_keywords = sorted(keyword_scores, key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "print(\"카테고리\", selected_category, \"추천 키워드:\")\n",
    "for kw, score in recommended_keywords:\n",
    "    print(f\"{kw}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
