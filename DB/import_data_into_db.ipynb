{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.40-cp39-cp39-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.1.1-cp39-cp39-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\hp\\.conda\\envs\\yt_project\\lib\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Downloading sqlalchemy-2.0.40-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 7.9 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.1.1-cp39-cp39-win_amd64.whl (298 kB)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.1.1 sqlalchemy-2.0.40\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mariadb\n",
      "  Downloading mariadb-1.1.12-cp39-cp39-win_amd64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\.conda\\envs\\yt_project\\lib\\site-packages (from mariadb) (24.2)\n",
      "Downloading mariadb-1.1.12-cp39-cp39-win_amd64.whl (203 kB)\n",
      "Installing collected packages: mariadb\n",
      "Successfully installed mariadb-1.1.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mariadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 행 개수: 101497\n",
      "중복된 videoID:\n",
      "             videoID  count\n",
      "1919    0FHkzeKz3Vs      2\n",
      "6023    2siR-j4ka6s      2\n",
      "6253    30r_XifbGoA      2\n",
      "6655    3IJEPlJ1Pn4      2\n",
      "8537    4T1lM9EmZb4      2\n",
      "...             ...    ...\n",
      "97776   xfoKFm_caIw      2\n",
      "98624   yEUcJsbLMoI      2\n",
      "100627  zVOUI0mUiWs      2\n",
      "100794  zanHKUv_ows      2\n",
      "100903  zfTjontofLg      2\n",
      "\n",
      "[101 rows x 2 columns]\n",
      "중복 제거 후 행 개수: 101396\n",
      "   categoryID  viewCount  likeCount  commentCount  duration\n",
      "0           1    10876.0      738.0          46.0    3912.0\n",
      "1           3      294.0        6.0           0.0     292.0\n",
      "2           1      888.0      115.0           2.0      59.0\n",
      "3           9    53186.0     2455.0         445.0     226.0\n",
      "4           1      446.0        8.0           0.0      46.0\n",
      "CSV 파일의 데이터가 'video' 테이블에 성공적으로 적재되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1. 환경변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "user = os.getenv('DB_USER', 'user1')\n",
    "password = os.getenv('DB_PASSWORD', 'yttest1234')\n",
    "host = os.getenv('DB_HOST', '121.128.172.79')\n",
    "port = os.getenv('DB_PORT', '3306')\n",
    "database = os.getenv('DB_NAME', 'yttest_db')\n",
    "\n",
    "port = int(port)\n",
    "\n",
    "# 2. CSV 파일 경로\n",
    "csv_file = 'C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/new_category_data_no_pos.csv'\n",
    "\n",
    "# 3. CSV 파일 읽기\n",
    "df = pd.read_csv(csv_file)\n",
    "print(\"CSV 행 개수:\", len(df))\n",
    "\n",
    "# videoID 기준 중복 개수 확인\n",
    "dup_counts = df.groupby('videoID').size().reset_index(name='count')\n",
    "duplicated_ids = dup_counts[dup_counts['count'] > 1]\n",
    "print(\"중복된 videoID:\\n\", duplicated_ids)\n",
    "\n",
    "df.drop_duplicates(subset=['videoID'], keep='first', inplace=True)\n",
    "print(\"중복 제거 후 행 개수:\", len(df))\n",
    "\n",
    "# 4. DB 테이블에 맞는 컬럼만 선택 및 순서 재정렬\n",
    "#    DB 테이블 'video'의 컬럼:\n",
    "#    [videoID, categoryID, channelID, title, viewCount, likeCount,\n",
    "#     commentCount, uploadDate, duration, tags, thumbnailURL, keyword]\n",
    "db_columns = [\n",
    "    'videoID',\n",
    "    'categoryID',\n",
    "    'channelID',\n",
    "    'title',\n",
    "    'viewCount',\n",
    "    'likeCount',\n",
    "    'commentCount',\n",
    "    'uploadDate',\n",
    "    'duration',\n",
    "    'tags',\n",
    "    'thumbnailURL',\n",
    "    'keyword'\n",
    "]\n",
    "\n",
    "# 불필요한 컬럼 제거\n",
    "df = df[db_columns]\n",
    "\n",
    "# 5. \"None\" 문자열을 결측치(NaN)로 변경 (숫자형 컬럼에서 문제가 될 수 있으므로)\n",
    "df.replace(\"None\", np.nan, inplace=True)\n",
    "\n",
    "# 7. 숫자형 컬럼 변환 (오류가 발생하는 경우, 예를 들어 categoryID, viewCount, likeCount, commentCount, duration)\n",
    "numeric_columns = ['categoryID', 'viewCount', 'likeCount', 'commentCount', 'duration']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 5. uploadDate 포맷 변환\n",
    "# ISO 포맷(예: '2025-02-21T14:29:59Z')을 datetime으로 변환 후, 시간대 정보를 제거하고 'YYYY-MM-DD HH:MM:SS' 형식으로 변환\n",
    "df['uploadDate'] = pd.to_datetime(df['uploadDate'], utc=True, errors='coerce')\n",
    "df['uploadDate'] = df['uploadDate'].dt.tz_localize(None)\n",
    "df['uploadDate'] = df['uploadDate'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# (변환 후, 각 숫자형 컬럼에 NaN이 포함되어 있는지 확인)\n",
    "print(df[numeric_columns].head())\n",
    "\n",
    "# 6. DB 연결을 위한 SQLAlchemy 엔진 생성\n",
    "engine = create_engine(\n",
    "    f\"mariadb+mariadbconnector://{user}:{password}@{host}:{port}/{database}\",\n",
    "    pool_recycle=3600\n",
    ")\n",
    "\n",
    "# 7. 외래 키 제약 조건 비활성화 후 데이터 삽입\n",
    "with engine.begin() as conn:\n",
    "    # 외래 키 제약 조건 비활성화\n",
    "    conn.execute(text(\"SET FOREIGN_KEY_CHECKS=0;\"))\n",
    "    # 데이터 삽입 (chunksize 옵션 사용)\n",
    "    df.to_sql('video', conn, if_exists='append', index=False, chunksize=1000)\n",
    "    # 외래 키 제약 조건 재활성화\n",
    "    conn.execute(text(\"SET FOREIGN_KEY_CHECKS=1;\"))\n",
    "\n",
    "print(\"CSV 파일의 데이터가 'video' 테이블에 성공적으로 적재되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1. 환경변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "user = os.getenv('DB_USER')\n",
    "password = os.getenv('DB_PASSWORD')\n",
    "host = os.getenv('DB_HOST')\n",
    "port = os.getenv('DB_PORT')\n",
    "database = os.getenv('DB_NAME')\n",
    "\n",
    "# 2. SQLAlchemy 엔진 생성\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{user}:{password}@{host}:{port}/{database}?charset=utf8mb4\"\n",
    ")\n",
    "\n",
    "# 3. 채널 데이터 불러오기 및 저장\n",
    "channel_df = pd.read_csv(\"channels_by_keywords_최종.csv\")\n",
    "\n",
    "# DB에 저장\n",
    "channel_df.to_sql(name='Channel', con=engine, if_exists='append', index=False)\n",
    "print(\"Channel 테이블에 데이터 삽입 완료\")\n",
    "\n",
    "# 4. 영상 데이터 불러오기\n",
    "video_df = pd.read_csv(\"videos_by_keywords_최종.csv\", encoding='utf-8')\n",
    "\n",
    "# segment 컬럼 제거 (있다면)\n",
    "if 'segment' in video_df.columns:\n",
    "    video_df = video_df.drop(columns=['segment'])\n",
    "\n",
    "# uploadDate 포맷 변환\n",
    "video_df['uploadDate'] = pd.to_datetime(video_df['uploadDate'], utc=True, errors='coerce')\n",
    "video_df['uploadDate'] = video_df['uploadDate'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# 컬럼명 정제\n",
    "video_df.columns = ['videoID', 'channelID', 'title', 'viewCount', 'likeCount', 'commentCount',\n",
    "                    'uploadDate', 'duration', 'tags', 'thumbnailURL', 'keyword', 'categoryID']\n",
    "\n",
    "# 5. 채널 ID 유효한 것만 필터링\n",
    "video_df = video_df.drop_duplicates(subset='videoID')\n",
    "with engine.connect() as conn:\n",
    "    valid_channels = pd.read_sql(\"SELECT channelID FROM Channel\", conn)\n",
    "valid_channel_ids = set(valid_channels['channelID'])\n",
    "\n",
    "video_df = video_df[video_df['channelID'].isin(valid_channel_ids)]\n",
    "print(f\"삽입 가능한 영상 수: {len(video_df)}\")\n",
    "\n",
    "# 6. DB에 저장\n",
    "video_df.to_sql(name='Video', con=engine, if_exists='append', index=False)\n",
    "print(\"Video 테이블에 데이터 삽입 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. channel 테이블에서 현재 존재하는 channelID 불러오기\n",
    "valid_channel_ids = pd.read_sql(\"SELECT channelID FROM channel\", con=engine)\n",
    "valid_ids_set = set(valid_channel_ids['channelID'])\n",
    "\n",
    "# 2. video_df에서 존재하지 않는 channelID만 필터링\n",
    "invalid_channel_df = video_df[~video_df['channelID'].isin(valid_ids_set)]\n",
    "\n",
    "# 3. 결과 확인\n",
    "print(f\"존재하지 않는 channelID 수: {invalid_channel_df['channelID'].nunique()}\")\n",
    "print(f\"해당되는 영상 수: {len(invalid_channel_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 환경 변수 불러오기\n",
    "load_dotenv()\n",
    "user = os.getenv('DB_USER')\n",
    "password = os.getenv('DB_PASSWORD')\n",
    "host = os.getenv('DB_HOST')\n",
    "port = os.getenv('DB_PORT')\n",
    "database = os.getenv('DB_NAME')\n",
    "\n",
    "# DB 엔진 생성\n",
    "engine = create_engine(f\"mysql+pymysql://{user}:{password}@{host}:{port}/{database}?charset=utf8mb4\")\n",
    "\n",
    "# CSV 파일 로드\n",
    "pos_df = pd.read_csv('pos_tagged_data.csv')\n",
    "\n",
    "# 컬럼 확인\n",
    "print(pos_df.columns)\n",
    "\n",
    "# videoID 기준 중복 제거\n",
    "pos_df = pos_df.drop_duplicates(subset='videoID')\n",
    "\n",
    "# DB 연결 후 업데이트\n",
    "with engine.begin() as conn:\n",
    "    for _, row in pos_df.iterrows():\n",
    "        video_id = row['videoID']\n",
    "        pos_tags = row['pos_tags']\n",
    "\n",
    "        # SQLAlchemy text 쿼리 + 딕셔너리 파라미터\n",
    "        conn.execute(\n",
    "            text(\"\"\"\n",
    "                UPDATE video\n",
    "                SET token_tagging = :token_tagging\n",
    "                WHERE videoID = :videoID\n",
    "            \"\"\"),\n",
    "            {\"token_tagging\": pos_tags, \"videoID\": video_id}\n",
    "        )\n",
    "\n",
    "print(\"video 테이블에 token_tagging 업데이트 완료.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YT_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
