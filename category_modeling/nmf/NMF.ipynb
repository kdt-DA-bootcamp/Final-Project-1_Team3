{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data Shape: (57039, 8)\n",
      "Topic #0: ì±Œë¦°ì§€ dance ì¶¤ì¶”ëŠ”ê³°ëŒ afstarz music kpop cola ì¶¤ì¶”ëŠ”ë‚˜ìœ¨ ì–´ê¹¨ëŒ„ìŠ¤ì±Œë¦°ì§€ ì‡¼ì¸ \n",
      "Topic #1: ì½”ë”” ë°ì¼ë¦¬ë£© íŒ¨ì…˜ ë£©ë¶ ootd fashion ë´„ì½”ë”” ìŠ¤íƒ€ì¼ë§ ë°ì¼ë¦¬ë£©ì½”ë”” ì¤‘ë…„íŒ¨ì…˜ì½”ë””\n",
      "Topic #2: ë¨¹ë°© mukbang asmr eating food korean ìš”ë¦¬ ë§›ì§‘ sound ë ˆì‹œí”¼\n",
      "Topic #3: ì¶”ì²œ top10 í›„ê¸° íŒë§¤ìˆœìœ„ ë¹„êµ ê°€ê²© í‰ì  ê°€ì¥ ë¦¬ë·° ìš”ì¦˜\n",
      "Topic #4: ë©”ì´í¬ì—… makeup makeuptutorial ë©”ì´í¬ì—…íŠœí† ë¦¬ì–¼ beauty tutorial grwm ë°ì¼ë¦¬ ëª¨ì¹´ë¬´ìŠ¤ ë·°í‹°\n",
      "Topic #5: ë¸Œì´ë¡œê·¸ vlog ì¼ìƒ ì•„ê¸° ì§ì¥ì¸ ì–¸ë°•ì‹± ì—¬í–‰ ìœ¡ì•„ ì¼ìƒë¸Œì´ë¡œê·¸ baby\n",
      "Topic #6: ì½˜ì„œíŠ¸ shorts 0100 6404 02 í–‰ì‚¬ì„­ì™¸ë¬¸ì˜ ê³µì—°í–‰ì‚¬ì„­ì™¸ë¬¸ì˜ ì„œì‚° ìœ¤í˜•ì£¼ 2025\n",
      "Topic #7: ì•„ì´ë¸Œ í¬ì¹´í¬ì¥ ì±Œë¦°ì§€ ì–¸ë°•ì‹± ive í¬ì¥ê³„ ë‹¤ì´ë¸Œ ì¶”ì²œ ì•„ì´ëŒ ì•Œê³ ë¦¬ì¦˜\n",
      "Topic #8: ë£©ë¶ lookbook ai 4k ì§ìº  ë€ì œë¦¬ ëª¨ë¸ underwear ë¹„í‚¤ë‹ˆ bikini\n",
      "Topic #9: ë§Œë“¤ê¸° ì¸í˜•ê³„ ë ˆì‹œí”¼ ì¸í˜• ë¹„ì¦ˆí‚¤ë§ diy ìš”ë¦¬ ê´€ë¦¬ìë‹˜ì¶”ì²œëœ¨ê²Œí•´ì£¼ì„¸ìš” í‚¤ë§ ì¸í˜•ê³„ë¸Œì´ë¡œê·¸\n",
      "NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_new.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
    "input_file = \"C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/preprocessed_data_unique.csv\"  # ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œ\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 1-1. categoryIDê°€ 6ì¸ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "df = df[df['categoryID'] == 6].reset_index(drop=True)\n",
    "print(\"Filtered Data Shape:\", df.shape)\n",
    "\n",
    "# 2. combined_text ì—´ ìƒì„±: ë§Œì•½ combined_textê°€ ì—†ìœ¼ë©´, title, tags, img_textë¥¼ ê²°í•©\n",
    "if \"combined_text\" not in df.columns:\n",
    "    # ê° ì—´ì˜ ê²°ì¸¡ì¹˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ì—¬ ê²°í•©\n",
    "    df[\"combined_text\"] = (\n",
    "        df[\"title\"].fillna(\"\") + \" \" +\n",
    "        df[\"tags\"].fillna(\"\") + \" \" +\n",
    "        df[\"img_text\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "# 3. ê²°í•©ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±\n",
    "docs = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# 4. TF-IDF í–‰ë ¬ ìƒì„±\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# 5. NMF ëª¨ë¸ í•™ìŠµ\n",
    "n_topics = 10  # ì›í•˜ëŠ” í† í”½ ìˆ˜\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf)  # ë¬¸ì„œ-í† í”½ í–‰ë ¬\n",
    "H = nmf_model.components_           # í† í”½-ë‹¨ì–´ í–‰ë ¬\n",
    "\n",
    "# 6. ê° í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics.append(\" \".join(top_words))\n",
    "    return topics\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topics = get_top_words(nmf_model, feature_names, n_top_words=10)\n",
    "\n",
    "# 7. í† í”½ ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ë¡œ ì €ì¥\n",
    "for i, topic in enumerate(topics):\n",
    "    print(\"Topic #{}: {}\".format(i, topic))\n",
    "\n",
    "with open(\"nmf_topics_new.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, topic in enumerate(topics):\n",
    "        f.write(\"Topic #{}: {}\\n\".format(i, topic))\n",
    "\n",
    "print(\"NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_new.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ì±Œë¦°ì§€ dance ì¶¤ì¶”ëŠ”ê³°ëŒ afstarz kpop music cola ì¶¤ì¶”ëŠ”ë‚˜ìœ¨ ì‡¼ì¸  ì–´ê¹¨ëŒ„ìŠ¤ì±Œë¦°ì§€\n",
      "Topic #1: ë¨¹ë°© mukbang asmr eating food korean ë§›ì§‘ ìš”ë¦¬ sound ë ˆì‹œí”¼\n",
      "Topic #2: í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ playlist ë…¸ë˜ ìŒì•… íŒì†¡ í”Œë¦¬ music ê°ì„± ğğ¥ğšğ²ğ¥ğ¢ğ¬ğ­ ì¢‹ì€\n",
      "Topic #3: ë£©ë¶ ì½”ë”” lookbook ë°ì¼ë¦¬ë£© ai íŒ¨ì…˜ ootd fashion ì¤‘ë…„íŒ¨ì…˜ì½”ë”” ë°ì¼ë¦¬ë£©ì½”ë””\n",
      "Topic #4: ë™ìš” ì–´ë¦°ì´ kids ë…¸ë˜ ì• ë‹ˆë©”ì´ì…˜ êµìœ¡ ì•„ê¸° ìœ ì•„ ìœ ì¹˜ì› songs\n",
      "Topic #5: ì¶”ì²œ ì˜í™” ë¦¬ë·° ë¹„êµ í›„ê¸° top10 ê°€ê²© íŒë§¤ìˆœìœ„ ìŠ¤ë§ˆíŠ¸í° ê°€ì¥\n",
      "Topic #6: ë©”ì´í¬ì—… makeup makeuptutorial ë©”ì´í¬ì—…íŠœí† ë¦¬ì–¼ ëª¨ì¹´ë¬´ìŠ¤ tutorial ë·°í‹° beauty shorts grwm\n",
      "Topic #7: ì§ìº  ì•„ì´ëŒ shorts ì¹˜ì–´ë¦¬ë” fancam ì½˜ì„œíŠ¸ 4k kpop í•˜ì´ë¼ì´íŠ¸ ë¼ì´ë¸Œ\n",
      "Topic #8: ë¸Œì´ë¡œê·¸ vlog ì¼ìƒ ì—¬í–‰ ì–¸ë°•ì‹± ì§ì¥ì¸ ì•„ê¸° ë§›ì§‘ ì¼ë³¸ ì¼ìƒë¸Œì´ë¡œê·¸\n",
      "Topic #9: ì˜ì–´ë‹¨ì–´ê³µë¶€ë²• ì˜ì–´ë‹¨ì–´ì™¸ìš°ê¸° ì˜ì–´ë‹¨ì–´ê³µë¶€ 5ì´ˆì•ˆì— ëŒ€ë‹µ 5ì´ˆ ëª°ë¼ ì˜ì–´ë¡œ ì´ê±° ê³µë¶€\n",
      "Topic #10: ë§Œë“¤ê¸° diy squishy nano tape ì‹¤ë¦¬ì½˜í…Œì´í”„ ì¸í˜•ê³„ ë§ë‘ì´ orbeez ë ˆì‹œí”¼\n",
      "NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_11.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
    "input_file = \"C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/preprocessed_data_comp.csv\"  # ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œ\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 2. combined_text ì—´ ìƒì„±: ë§Œì•½ combined_textê°€ ì—†ìœ¼ë©´, title, tags, img_textë¥¼ ê²°í•©\n",
    "if \"combined_text\" not in df.columns:\n",
    "    # ê° ì—´ì˜ ê²°ì¸¡ì¹˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ì—¬ ê²°í•©\n",
    "    df[\"combined_text\"] = (\n",
    "        df[\"title\"].fillna(\"\") + \" \" +\n",
    "        df[\"tags\"].fillna(\"\") + \" \" +\n",
    "        df[\"img_text\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "# 3. ê²°í•©ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±\n",
    "docs = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# 4. TF-IDF í–‰ë ¬ ìƒì„±\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# 5. NMF ëª¨ë¸ í•™ìŠµ\n",
    "n_topics = 11  # ì›í•˜ëŠ” í† í”½ ìˆ˜\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf)  # ë¬¸ì„œ-í† í”½ í–‰ë ¬\n",
    "H = nmf_model.components_           # í† í”½-ë‹¨ì–´ í–‰ë ¬\n",
    "\n",
    "# 6. ê° í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics.append(\" \".join(top_words))\n",
    "    return topics\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topics = get_top_words(nmf_model, feature_names, n_top_words=10)\n",
    "\n",
    "# 7. í† í”½ ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ë¡œ ì €ì¥\n",
    "for i, topic in enumerate(topics):\n",
    "    print(\"Topic #{}: {}\".format(i, topic))\n",
    "\n",
    "    with open(\"nmf_topics_11.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, topic in enumerate(topics):\n",
    "            f.write(\"Topic #{}: {}\\n\".format(i, topic))\n",
    "\n",
    "print(\"NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_11.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ì±Œë¦°ì§€ dance ì¶¤ì¶”ëŠ”ê³°ëŒ afstarz kpop music cola ì‡¼ì¸  ì¶¤ì¶”ëŠ”ë‚˜ìœ¨ ì–´ê¹¨ëŒ„ìŠ¤ì±Œë¦°ì§€\n",
      "Topic #1: ë¨¹ë°© mukbang asmr eating food korean ë§›ì§‘ show ìš”ë¦¬ sound\n",
      "Topic #2: í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ playlist ë…¸ë˜ ìŒì•… íŒì†¡ í”Œë¦¬ music ê°ì„± ğğ¥ğšğ²ğ¥ğ¢ğ¬ğ­ ì¢‹ì€\n",
      "Topic #3: ë£©ë¶ ì½”ë”” lookbook ë°ì¼ë¦¬ë£© ai íŒ¨ì…˜ fashion ootd ì¤‘ë…„íŒ¨ì…˜ì½”ë”” ë°ì¼ë¦¬ë£©ì½”ë””\n",
      "Topic #4: ë™ìš” ì–´ë¦°ì´ kids ë…¸ë˜ ì• ë‹ˆë©”ì´ì…˜ êµìœ¡ ì•„ê¸° ìœ ì•„ ìœ ì¹˜ì› songs\n",
      "Topic #5: ì¶”ì²œ ë¹„êµ top10 í›„ê¸° ë¦¬ë·° ê°€ê²© íŒë§¤ìˆœìœ„ ìŠ¤ë§ˆíŠ¸í° ê°€ì¥ top\n",
      "Topic #6: ë©”ì´í¬ì—… makeup makeuptutorial ë©”ì´í¬ì—…íŠœí† ë¦¬ì–¼ ëª¨ì¹´ë¬´ìŠ¤ tutorial shorts ë·°í‹° beauty grwm\n",
      "Topic #7: ì§ìº  ì•„ì´ëŒ shorts ì¹˜ì–´ë¦¬ë” ì½˜ì„œíŠ¸ fancam 4k kpop 2025 ë¼ì´ë¸Œ\n",
      "Topic #8: ë¸Œì´ë¡œê·¸ vlog ì¼ìƒ ì—¬í–‰ ì–¸ë°•ì‹± ì§ì¥ì¸ ì•„ê¸° ë§›ì§‘ ì¼ë³¸ ì¼ìƒë¸Œì´ë¡œê·¸\n",
      "Topic #9: ì˜ì–´ë‹¨ì–´ê³µë¶€ë²• ì˜ì–´ë‹¨ì–´ì™¸ìš°ê¸° ì˜ì–´ë‹¨ì–´ê³µë¶€ 5ì´ˆì•ˆì— ëŒ€ë‹µ 5ì´ˆ ëª°ë¼ ì˜ì–´ë¡œ ì´ê±° ê³µë¶€\n",
      "Topic #10: ë§Œë“¤ê¸° diy squishy nano tape with ì‹¤ë¦¬ì½˜í…Œì´í”„ ë§ë‘ì´ ì¸í˜•ê³„ orbeez\n",
      "Topic #11: ì˜í™” ë“œë¼ë§ˆ ëª…ì¥ë©´ ë°°ìš° ë„·í”Œë¦­ìŠ¤ ê²°ë§í¬í•¨ ë¦¬ë·° ì˜í™”ë¦¬ë·° shorts ì˜í™”ì¶”ì²œ\n",
      "NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_12.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
    "input_file = \"C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/preprocessed_data_comp.csv\"  # ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œ\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 2. combined_text ì—´ ìƒì„±: ë§Œì•½ combined_textê°€ ì—†ìœ¼ë©´, title, tags, img_textë¥¼ ê²°í•©\n",
    "if \"combined_text\" not in df.columns:\n",
    "    # ê° ì—´ì˜ ê²°ì¸¡ì¹˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ì—¬ ê²°í•©\n",
    "    df[\"combined_text\"] = (\n",
    "        df[\"title\"].fillna(\"\") + \" \" +\n",
    "        df[\"tags\"].fillna(\"\") + \" \" +\n",
    "        df[\"img_text\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "# 3. ê²°í•©ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±\n",
    "docs = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# 4. TF-IDF í–‰ë ¬ ìƒì„±\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# 5. NMF ëª¨ë¸ í•™ìŠµ\n",
    "n_topics = 12  # ì›í•˜ëŠ” í† í”½ ìˆ˜\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf)  # ë¬¸ì„œ-í† í”½ í–‰ë ¬\n",
    "H = nmf_model.components_           # í† í”½-ë‹¨ì–´ í–‰ë ¬\n",
    "\n",
    "# 6. ê° í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics.append(\" \".join(top_words))\n",
    "    return topics\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topics = get_top_words(nmf_model, feature_names, n_top_words=10)\n",
    "\n",
    "# 7. í† í”½ ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ë¡œ ì €ì¥\n",
    "for i, topic in enumerate(topics):\n",
    "    print(\"Topic #{}: {}\".format(i, topic))\n",
    "\n",
    "    with open(\"nmf_topics_12.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, topic in enumerate(topics):\n",
    "            f.write(\"Topic #{}: {}\\n\".format(i, topic))\n",
    "\n",
    "print(\"NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_12.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ì±Œë¦°ì§€ dance ì¶¤ì¶”ëŠ”ê³°ëŒ afstarz kpop music cola ì‡¼ì¸  ì¶¤ì¶”ëŠ”ë‚˜ìœ¨ ì–´ê¹¨ëŒ„ìŠ¤ì±Œë¦°ì§€\n",
      "Topic #1: ë¨¹ë°© mukbang asmr eating food korean ë§›ì§‘ ìš”ë¦¬ sound ë ˆì‹œí”¼\n",
      "Topic #2: í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ playlist ë…¸ë˜ ìŒì•… íŒì†¡ í”Œë¦¬ music ê°ì„± ğğ¥ğšğ²ğ¥ğ¢ğ¬ğ­ ì¢‹ì€\n",
      "Topic #3: ë£©ë¶ ì½”ë”” lookbook ë°ì¼ë¦¬ë£© ai íŒ¨ì…˜ ootd fashion ì¤‘ë…„íŒ¨ì…˜ì½”ë”” ë°ì¼ë¦¬ë£©ì½”ë””\n",
      "Topic #4: ë™ìš” ì–´ë¦°ì´ kids ë…¸ë˜ ì• ë‹ˆë©”ì´ì…˜ êµìœ¡ ì•„ê¸° ìœ ì•„ ìœ ì¹˜ì› songs\n",
      "Topic #5: ì¶”ì²œ ë¹„êµ top10 í›„ê¸° ë¦¬ë·° íŒë§¤ìˆœìœ„ ê°€ê²© ìŠ¤ë§ˆíŠ¸í° ê°€ì¥ í‰ì \n",
      "Topic #6: ë©”ì´í¬ì—… makeup makeuptutorial ë©”ì´í¬ì—…íŠœí† ë¦¬ì–¼ ëª¨ì¹´ë¬´ìŠ¤ shorts tutorial beauty ë·°í‹° grwm\n",
      "Topic #7: ì§ìº  ì•„ì´ëŒ ì¹˜ì–´ë¦¬ë” fancam 4k kpop ì•„ì´ë¸Œ shorts ê³µì—° í•˜ì¸ íˆ¬í•˜ì¸ \n",
      "Topic #8: ë¸Œì´ë¡œê·¸ vlog ì¼ìƒ ì—¬í–‰ ì–¸ë°•ì‹± ì§ì¥ì¸ ì•„ê¸° ë§›ì§‘ ì¼ë³¸ ì¼ìƒë¸Œì´ë¡œê·¸\n",
      "Topic #9: ì˜ì–´ë‹¨ì–´ê³µë¶€ë²• ì˜ì–´ë‹¨ì–´ì™¸ìš°ê¸° ì˜ì–´ë‹¨ì–´ê³µë¶€ 5ì´ˆì•ˆì— ëŒ€ë‹µ 5ì´ˆ ëª°ë¼ ì˜ì–´ë¡œ ì´ê±° ê³µë¶€\n",
      "Topic #10: ë§Œë“¤ê¸° diy squishy nano tape ì‹¤ë¦¬ì½˜í…Œì´í”„ ì¸í˜•ê³„ ë§ë‘ì´ orbeez ë ˆì‹œí”¼\n",
      "Topic #11: ì½˜ì„œíŠ¸ 2025 í•˜ì´ë¼ì´íŠ¸ 02 0100 6404 shorts í–‰ì‚¬ì„­ì™¸ë¬¸ì˜ vs í”Œë ˆì´ë¸Œ\n",
      "Topic #12: ì˜í™” ë“œë¼ë§ˆ ëª…ì¥ë©´ ë°°ìš° ë„·í”Œë¦­ìŠ¤ ê²°ë§í¬í•¨ ë¦¬ë·° ì˜í™”ë¦¬ë·° shorts ì˜í™”ì¶”ì²œ\n",
      "NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_13.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "NMF ëª¨ë¸ê³¼ TF-IDF ë²¡í„°ë¼ì´ì €ê°€ ê°ê° 'nmf_model_13.pkl'ê³¼ 'tfidf_vectorizer_13.pkl' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
    "input_file = \"C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/preprocessed_data_comp.csv\"  # ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œ\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 2. combined_text ì—´ ìƒì„±: ë§Œì•½ combined_textê°€ ì—†ìœ¼ë©´, title, tags, img_textë¥¼ ê²°í•©\n",
    "if \"combined_text\" not in df.columns:\n",
    "    # ê° ì—´ì˜ ê²°ì¸¡ì¹˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ì—¬ ê²°í•©\n",
    "    df[\"combined_text\"] = (\n",
    "        df[\"title\"].fillna(\"\") + \" \" +\n",
    "        df[\"tags\"].fillna(\"\") + \" \" +\n",
    "        df[\"img_text\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "# 3. ê²°í•©ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±\n",
    "docs = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# 4. TF-IDF í–‰ë ¬ ìƒì„±\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# 5. NMF ëª¨ë¸ í•™ìŠµ\n",
    "n_topics = 13  # ì›í•˜ëŠ” í† í”½ ìˆ˜\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf)  # ë¬¸ì„œ-í† í”½ í–‰ë ¬\n",
    "H = nmf_model.components_           # í† í”½-ë‹¨ì–´ í–‰ë ¬\n",
    "\n",
    "# 6. ê° í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics.append(\" \".join(top_words))\n",
    "    return topics\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topics = get_top_words(nmf_model, feature_names, n_top_words=10)\n",
    "\n",
    "# 7. í† í”½ ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ë¡œ ì €ì¥\n",
    "for i, topic in enumerate(topics):\n",
    "    print(\"Topic #{}: {}\".format(i, topic))\n",
    "\n",
    "    with open(\"nmf_topics_13.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, topic in enumerate(topics):\n",
    "            f.write(\"Topic #{}: {}\\n\".format(i, topic))\n",
    "\n",
    "print(\"NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_13.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 8. ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì € ì €ì¥\n",
    "with open(\"nmf_model_13.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(nmf_model, model_file)\n",
    "with open(\"tfidf_vectorizer_13.pkl\", \"wb\") as vec_file:\n",
    "    pickle.dump(tfidf_vectorizer, vec_file)\n",
    "\n",
    "print(\"NMF ëª¨ë¸ê³¼ TF-IDF ë²¡í„°ë¼ì´ì €ê°€ ê°ê° 'nmf_model_13.pkl'ê³¼ 'tfidf_vectorizer_13.pkl' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ì±Œë¦°ì§€ dance ì¶¤ì¶”ëŠ”ê³°ëŒ afstarz kpop music cola ì‡¼ì¸  ì¶¤ì¶”ëŠ”ë‚˜ìœ¨ ì–´ê¹¨ëŒ„ìŠ¤ì±Œë¦°ì§€\n",
      "Topic #1: ë¨¹ë°© mukbang asmr eating food korean ë§›ì§‘ ìš”ë¦¬ sound ë ˆì‹œí”¼\n",
      "Topic #2: í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ playlist ë…¸ë˜ ìŒì•… íŒì†¡ í”Œë¦¬ music ê°ì„± ğğ¥ğšğ²ğ¥ğ¢ğ¬ğ­ ì¢‹ì€\n",
      "Topic #3: ì½”ë”” ë°ì¼ë¦¬ë£© íŒ¨ì…˜ ootd ì¤‘ë…„íŒ¨ì…˜ì½”ë”” fashion ë£©ë¶ ìŠ¤íƒ€ì¼ë§ ë°ì¼ë¦¬ë£©ì½”ë”” ë´„ì½”ë””\n",
      "Topic #4: ë™ìš” ì–´ë¦°ì´ kids ë…¸ë˜ ì• ë‹ˆë©”ì´ì…˜ êµìœ¡ ì•„ê¸° ìœ ì•„ ìœ ì¹˜ì› songs\n",
      "Topic #5: ì¶”ì²œ ë¹„êµ top10 í›„ê¸° ë¦¬ë·° íŒë§¤ìˆœìœ„ ê°€ê²© ìŠ¤ë§ˆíŠ¸í° ê°€ì¥ í‰ì \n",
      "Topic #6: ë©”ì´í¬ì—… makeup makeuptutorial ë©”ì´í¬ì—…íŠœí† ë¦¬ì–¼ ëª¨ì¹´ë¬´ìŠ¤ shorts tutorial beauty ë·°í‹° grwm\n",
      "Topic #7: ì§ìº  ì¹˜ì–´ë¦¬ë” ì½˜ì„œíŠ¸ fancam shorts 2025 í•˜ì´ë¼ì´íŠ¸ 4k 02 ë¼ì´ë¸Œ\n",
      "Topic #8: ë¸Œì´ë¡œê·¸ vlog ì¼ìƒ ì—¬í–‰ ì§ì¥ì¸ ì•„ê¸° ë§›ì§‘ ì¼ë³¸ ì¼ìƒë¸Œì´ë¡œê·¸ ìœ¡ì•„\n",
      "Topic #9: ì˜ì–´ë‹¨ì–´ê³µë¶€ë²• ì˜ì–´ë‹¨ì–´ì™¸ìš°ê¸° ì˜ì–´ë‹¨ì–´ê³µë¶€ 5ì´ˆì•ˆì— ëŒ€ë‹µ 5ì´ˆ ëª°ë¼ ì˜ì–´ë¡œ ì´ê±° ê³µë¶€\n",
      "Topic #10: ë§Œë“¤ê¸° diy squishy nano tape ì‹¤ë¦¬ì½˜í…Œì´í”„ ì¸í˜•ê³„ ë§ë‘ì´ orbeez ë ˆì‹œí”¼\n",
      "Topic #11: ì˜í™” ë“œë¼ë§ˆ ëª…ì¥ë©´ ë°°ìš° ë„·í”Œë¦­ìŠ¤ ê²°ë§í¬í•¨ ë¦¬ë·° ì˜í™”ë¦¬ë·° shorts ì˜í™”ì¶”ì²œ\n",
      "Topic #12: ë£©ë¶ lookbook ai ë€ì œë¦¬ 4k ëª¨ë¸ bikini ë¹„í‚¤ë‹ˆ underwear ì„¸ë¡œë£©ë¶\n",
      "Topic #13: ì–¸ë°•ì‹± unboxing asmr í”¼ê·œì–´ ì•„ì´ë¸Œ ì„ ë¬¼ ì•„ì´í° í•˜ìš¸ shorts ì•¨ë²”\n",
      "Topic #14: ì•„ì´ëŒ í”Œë ˆì´ë¸Œ ì•„ì´ë¸Œ plave kpop ì¼€ì´íŒ ive ë°¤ë¹„ í•˜ë¯¼ í¬ì¹´í¬ì¥\n",
      "NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_14.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
    "input_file = \"C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/preprocessed_data_comp.csv\"  # ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œ\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 2. combined_text ì—´ ìƒì„±: ë§Œì•½ combined_textê°€ ì—†ìœ¼ë©´, title, tags, img_textë¥¼ ê²°í•©\n",
    "if \"combined_text\" not in df.columns:\n",
    "    # ê° ì—´ì˜ ê²°ì¸¡ì¹˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ì—¬ ê²°í•©\n",
    "    df[\"combined_text\"] = (\n",
    "        df[\"title\"].fillna(\"\") + \" \" +\n",
    "        df[\"tags\"].fillna(\"\") + \" \" +\n",
    "        df[\"img_text\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "# 3. ê²°í•©ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±\n",
    "docs = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# 4. TF-IDF í–‰ë ¬ ìƒì„±\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# 5. NMF ëª¨ë¸ í•™ìŠµ\n",
    "n_topics = 15  # ì›í•˜ëŠ” í† í”½ ìˆ˜\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf)  # ë¬¸ì„œ-í† í”½ í–‰ë ¬\n",
    "H = nmf_model.components_           # í† í”½-ë‹¨ì–´ í–‰ë ¬\n",
    "\n",
    "# 6. ê° í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics.append(\" \".join(top_words))\n",
    "    return topics\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topics = get_top_words(nmf_model, feature_names, n_top_words=10)\n",
    "\n",
    "# 7. í† í”½ ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ë¡œ ì €ì¥\n",
    "for i, topic in enumerate(topics):\n",
    "    print(\"Topic #{}: {}\".format(i, topic))\n",
    "\n",
    "    with open(\"nmf_topics_14.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, topic in enumerate(topics):\n",
    "            f.write(\"Topic #{}: {}\\n\".format(i, topic))\n",
    "\n",
    "print(\"NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_14.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ì±Œë¦°ì§€ dance ì¶¤ì¶”ëŠ”ê³°ëŒ afstarz kpop music cola ì‡¼ì¸  ì¶¤ì¶”ëŠ”ë‚˜ìœ¨ ì–´ê¹¨ëŒ„ìŠ¤ì±Œë¦°ì§€\n",
      "Topic #1: ë¨¹ë°© mukbang asmr eating food korean ë§›ì§‘ ìš”ë¦¬ sound ë ˆì‹œí”¼\n",
      "Topic #2: í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ playlist ë…¸ë˜ ìŒì•… íŒì†¡ í”Œë¦¬ music ê°ì„± ğğ¥ğšğ²ğ¥ğ¢ğ¬ğ­ ì¢‹ì€\n",
      "Topic #3: ì½”ë”” ë°ì¼ë¦¬ë£© íŒ¨ì…˜ ootd ì¤‘ë…„íŒ¨ì…˜ì½”ë”” fashion ë£©ë¶ ìŠ¤íƒ€ì¼ë§ ë°ì¼ë¦¬ë£©ì½”ë”” ë´„ì½”ë””\n",
      "Topic #4: ë™ìš” ì–´ë¦°ì´ kids ë…¸ë˜ ì• ë‹ˆë©”ì´ì…˜ êµìœ¡ ì•„ê¸° ìœ ì•„ ìœ ì¹˜ì› songs\n",
      "Topic #5: ì¶”ì²œ ë¹„êµ top10 í›„ê¸° ë¦¬ë·° íŒë§¤ìˆœìœ„ ê°€ê²© ìŠ¤ë§ˆíŠ¸í° ê°€ì¥ í‰ì \n",
      "Topic #6: ë©”ì´í¬ì—… makeup makeuptutorial ë©”ì´í¬ì—…íŠœí† ë¦¬ì–¼ ëª¨ì¹´ë¬´ìŠ¤ shorts tutorial beauty ë·°í‹° grwm\n",
      "Topic #7: ì§ìº  ì¹˜ì–´ë¦¬ë” ì½˜ì„œíŠ¸ fancam shorts 2025 í•˜ì´ë¼ì´íŠ¸ 4k 02 ë¼ì´ë¸Œ\n",
      "Topic #8: ë¸Œì´ë¡œê·¸ vlog ì¼ìƒ ì—¬í–‰ ì§ì¥ì¸ ì•„ê¸° ë§›ì§‘ ì¼ë³¸ ì¼ìƒë¸Œì´ë¡œê·¸ ìœ¡ì•„\n",
      "Topic #9: ì˜ì–´ë‹¨ì–´ê³µë¶€ë²• ì˜ì–´ë‹¨ì–´ì™¸ìš°ê¸° ì˜ì–´ë‹¨ì–´ê³µë¶€ 5ì´ˆì•ˆì— ëŒ€ë‹µ 5ì´ˆ ëª°ë¼ ì˜ì–´ë¡œ ì´ê±° ê³µë¶€\n",
      "Topic #10: ë§Œë“¤ê¸° diy squishy nano tape ì‹¤ë¦¬ì½˜í…Œì´í”„ ì¸í˜•ê³„ ë§ë‘ì´ orbeez ë ˆì‹œí”¼\n",
      "Topic #11: ì˜í™” ë“œë¼ë§ˆ ëª…ì¥ë©´ ë°°ìš° ë„·í”Œë¦­ìŠ¤ ê²°ë§í¬í•¨ ë¦¬ë·° ì˜í™”ë¦¬ë·° shorts ì˜í™”ì¶”ì²œ\n",
      "Topic #12: ë£©ë¶ lookbook ai ë€ì œë¦¬ 4k ëª¨ë¸ bikini ë¹„í‚¤ë‹ˆ underwear ì„¸ë¡œë£©ë¶\n",
      "Topic #13: ì–¸ë°•ì‹± unboxing asmr í”¼ê·œì–´ ì•„ì´ë¸Œ ì„ ë¬¼ ì•„ì´í° í•˜ìš¸ shorts ì•¨ë²”\n",
      "Topic #14: ì•„ì´ëŒ í”Œë ˆì´ë¸Œ ì•„ì´ë¸Œ plave kpop ì¼€ì´íŒ ive ë°¤ë¹„ í•˜ë¯¼ í¬ì¹´í¬ì¥\n",
      "NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_15.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "NMF ëª¨ë¸ê³¼ TF-IDF ë²¡í„°ë¼ì´ì €ê°€ ê°ê° 'nmf_model_15.pkl'ê³¼ 'tfidf_vectorizer_15.pkl' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
    "input_file = \"C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/preprocessed_data_comp.csv\"  # ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œ\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 2. combined_text ì—´ ìƒì„±: ë§Œì•½ combined_textê°€ ì—†ìœ¼ë©´, title, tags, img_textë¥¼ ê²°í•©\n",
    "if \"combined_text\" not in df.columns:\n",
    "    # ê° ì—´ì˜ ê²°ì¸¡ì¹˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ì—¬ ê²°í•©\n",
    "    df[\"combined_text\"] = (\n",
    "        df[\"title\"].fillna(\"\") + \" \" +\n",
    "        df[\"tags\"].fillna(\"\") + \" \" +\n",
    "        df[\"img_text\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "# 3. ê²°í•©ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±\n",
    "docs = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# 4. TF-IDF í–‰ë ¬ ìƒì„±\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# 5. NMF ëª¨ë¸ í•™ìŠµ\n",
    "n_topics = 15  # ì›í•˜ëŠ” í† í”½ ìˆ˜\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf)  # ë¬¸ì„œ-í† í”½ í–‰ë ¬\n",
    "H = nmf_model.components_           # í† í”½-ë‹¨ì–´ í–‰ë ¬\n",
    "\n",
    "# 6. ê° í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics.append(\" \".join(top_words))\n",
    "    return topics\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topics = get_top_words(nmf_model, feature_names, n_top_words=10)\n",
    "\n",
    "# 7. í† í”½ ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ë¡œ ì €ì¥\n",
    "for i, topic in enumerate(topics):\n",
    "    print(\"Topic #{}: {}\".format(i, topic))\n",
    "\n",
    "    with open(\"nmf_topics_15.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, topic in enumerate(topics):\n",
    "            f.write(\"Topic #{}: {}\\n\".format(i, topic))\n",
    "\n",
    "print(\"NMF í† í”½ ëª¨ë¸ë§ ê²°ê³¼ê°€ 'nmf_topics_15.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 8. ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì € ì €ì¥\n",
    "with open(\"nmf_model_15.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(nmf_model, model_file)\n",
    "with open(\"tfidf_vectorizer_15.pkl\", \"wb\") as vec_file:\n",
    "    pickle.dump(tfidf_vectorizer, vec_file)\n",
    "\n",
    "print(\"NMF ëª¨ë¸ê³¼ TF-IDF ë²¡í„°ë¼ì´ì €ê°€ ê°ê° 'nmf_model_15.pkl'ê³¼ 'tfidf_vectorizer_15.pkl' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ì¶¤ì¶”ëŠ”ê³°ëŒ afstarz dance cola music ì¶¤ì¶”ëŠ”ë‚˜ìœ¨ kpop ì–´ê¹¨ëŒ„ìŠ¤ì±Œë¦°ì§€ ì¶¤ì¶”ëŠ”ë‚˜ìœ¨cola ì‡¼ì¸ \n",
      "Topic #1: ë¨¹ë°© mukbang asmr eating food korean sound spicy ë¦¬ì–¼ì‚¬ìš´ë“œ real\n",
      "Topic #2: í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ playlist ë…¸ë˜ ìŒì•… íŒì†¡ í”Œë¦¬ music ê°ì„± ğğ¥ğšğ²ğ¥ğ¢ğ¬ğ­ ì¢‹ì€\n",
      "Topic #3: ì½”ë”” ë°ì¼ë¦¬ë£© íŒ¨ì…˜ ootd ì¤‘ë…„íŒ¨ì…˜ì½”ë”” fashion ë£©ë¶ ìŠ¤íƒ€ì¼ë§ ë°ì¼ë¦¬ë£©ì½”ë”” ë´„ì½”ë””\n",
      "Topic #4: ë™ìš” ì–´ë¦°ì´ kids ë…¸ë˜ ì• ë‹ˆë©”ì´ì…˜ êµìœ¡ ìœ ì•„ ìœ ì¹˜ì› ì•„ê¸° songs\n",
      "Topic #5: ì¶”ì²œ ë¹„êµ top10 í›„ê¸° íŒë§¤ìˆœìœ„ ê°€ê²© ë¦¬ë·° ìŠ¤ë§ˆíŠ¸í° ê°€ì¥ í‰ì \n",
      "Topic #6: ë©”ì´í¬ì—… makeup makeuptutorial ë©”ì´í¬ì—…íŠœí† ë¦¬ì–¼ ëª¨ì¹´ë¬´ìŠ¤ tutorial ë·°í‹° beauty ë°ì¼ë¦¬ grwm\n",
      "Topic #7: ë£©ë¶ lookbook ai ë€ì œë¦¬ 4k ëª¨ë¸ bikini ë¹„í‚¤ë‹ˆ underwear ì„¸ë¡œë£©ë¶\n",
      "Topic #8: ë¸Œì´ë¡œê·¸ vlog ì¼ìƒ ì§ì¥ì¸ ì—¬í–‰ ì•„ê¸° ì¼ìƒë¸Œì´ë¡œê·¸ ìœ¡ì•„ ì»¤í”Œ ì¼ë³¸\n",
      "Topic #9: ì˜ì–´ë‹¨ì–´ê³µë¶€ë²• ì˜ì–´ë‹¨ì–´ì™¸ìš°ê¸° ì˜ì–´ë‹¨ì–´ê³µë¶€ 5ì´ˆì•ˆì— ëŒ€ë‹µ 5ì´ˆ ëª°ë¼ ì˜ì–´ë¡œ ì´ê±° ê³µë¶€\n",
      "Topic #10: ë§Œë“¤ê¸° diy squishy nano tape ì‹¤ë¦¬ì½˜í…Œì´í”„ ë§ë‘ì´ ì¸í˜•ê³„ orbeez ë³¼íœ\n",
      "Topic #11: shorts ì•„ê¸° ì‡¼ì¸  ë†€ì´ baby ì»¤ë²„ cute cover ìœ¡ì•„ ë“œë¼ë§ˆ\n",
      "Topic #12: ì˜í™” ë“œë¼ë§ˆ ëª…ì¥ë©´ ë„·í”Œë¦­ìŠ¤ ê²°ë§í¬í•¨ ë°°ìš° ë¦¬ë·° ì˜í™”ë¦¬ë·° ì˜í™”ì¶”ì²œ ê²°ë§\n",
      "Topic #13: ì½˜ì„œíŠ¸ 0100 6404 02 í–‰ì‚¬ì„­ì™¸ë¬¸ì˜ ê³µì—°í–‰ì‚¬ì„­ì™¸ë¬¸ì˜ ë°•ì§€í˜„ ìœ¤í˜•ì£¼ íŠ¸ë¡œíŠ¸ ì œë‹ˆ\n",
      "Topic #14: ì–¸ë°•ì‹± unboxing asmr í”¼ê·œì–´ ì•„ì´ë¸Œ ì„ ë¬¼ ì•„ì´í° í•˜ìš¸ ë¦¬ë·° ì•¨ë²”\n",
      "Topic #15: ì±Œë¦°ì§€ ì•„ì´ë¸Œ challenge í¬ì¹´í¬ì¥ ëŒ„ìŠ¤ ive í¬ì¥ê³„ dance í¬ì¹´ì±Œë¦°ì§€ í‹±í†¡\n",
      "Topic #16: í•˜ì´ë¼ì´íŠ¸ 2025 vs 03 ì¼ì • 25 live ë¼ì´ë¸Œ ê³„íš 10\n",
      "Topic #17: ì§ìº  ì•„ì´ëŒ fancam ì¹˜ì–´ë¦¬ë” 4k kpop í•˜ì¸ íˆ¬í•˜ì¸  ê³µì—° pop mpdì§ìº \n",
      "Topic #18: ë§›ì§‘ ë ˆì‹œí”¼ ì—¬í–‰ ìš”ë¦¬ food ì§‘ë°¥ cooking ì¼ë³¸ ë‹¤ì´ì–´íŠ¸ ìì·¨ìš”ë¦¬\n",
      "Topic #19: í”Œë ˆì´ë¸Œ plave ë°¤ë¹„ í•˜ë¯¼ ì˜ˆì¤€ ì€í˜¸ ë…¸ì•„ dash 2ì£¼ë…„ hamin\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
    "input_file = \"C:/Users/hp/Desktop/Bootcamp/PROJECT_OTT_AARRR/preprocessed_data_comp.csv\"  # ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œ\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 2. combined_text ì—´ ìƒì„±: ë§Œì•½ combined_textê°€ ì—†ìœ¼ë©´, title, tags, img_textë¥¼ ê²°í•©\n",
    "if \"combined_text\" not in df.columns:\n",
    "    # ê° ì—´ì˜ ê²°ì¸¡ì¹˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ì—¬ ê²°í•©\n",
    "    df[\"combined_text\"] = (\n",
    "        df[\"title\"].fillna(\"\") + \" \" +\n",
    "        df[\"tags\"].fillna(\"\") + \" \" +\n",
    "        df[\"img_text\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "# 3. ê²°í•©ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±\n",
    "docs = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# 4. TF-IDF í–‰ë ¬ ìƒì„±\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# 5. NMF ëª¨ë¸ í•™ìŠµ\n",
    "n_topics = 20  # ì›í•˜ëŠ” í† í”½ ìˆ˜\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf)  # ë¬¸ì„œ-í† í”½ í–‰ë ¬\n",
    "H = nmf_model.components_           # í† í”½-ë‹¨ì–´ í–‰ë ¬\n",
    "\n",
    "# 6. ê° í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics.append(\" \".join(top_words))\n",
    "    return topics\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topics = get_top_words(nmf_model, feature_names, n_top_words=10)\n",
    "\n",
    "# 7. í† í”½ ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ë¡œ ì €ì¥\n",
    "for i, topic in enumerate(topics):\n",
    "    print(\"Topic #{}: {}\".format(i, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YT_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
